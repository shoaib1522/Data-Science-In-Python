{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Data Analysis for Research Center\n",
    "\n",
    "## Assignment Overview\n",
    "In this assignment, we are tasked with analyzing climate data collected from 500 locations over the span of one year. The data includes daily **temperature** and **humidity** values. As data scientists, our goal is to perform several analyses to uncover climate trends, seasonal patterns, and other metrics that can support climate research efforts.\n",
    "\n",
    "### Goals of Each Task\n",
    "1. **Initialize Temperature and Humidity Data**: Create arrays to store temperature (in Celsius) and humidity values for each location across 365 days.\n",
    "2. **Simulate and Check Missing Data**: Introduce missing values randomly and calculate the total number of missing entries.\n",
    "3. **Convert Temperature and Calculate Discomfort Index**: Convert temperatures to Fahrenheit and compute a \"feels like\" index to represent perceived discomfort.\n",
    "4. **Analyze January Temperatures**: Extract and analyze data specific to January, calculating the average temperature.\n",
    "5. **Identify Extreme Temperatures**: Mark potentially erroneous data by replacing extreme temperatures with `NaN` values and count them per location.\n",
    "6. **Calculate Quarterly Temperature Averages**: Calculate the average temperature per season by dividing the year into four quarters.\n",
    "7. **Classify Humidity Levels**: Categorize daily humidity as \"Dry\" or \"Humid\" and count occurrences per location.\n",
    "8. **Apply Daily Pressure Trend**: Simulate atmospheric pressure changes and apply a trend adjustment to temperature data.\n",
    "\n",
    "### Instructions\n",
    "Each task is implemented in its own cell with comments and explanations provided throughout the notebook to document the approach and methodology used.\n",
    "\n",
    "---\n",
    "**Note**: Vectorized operations are used wherever possible to improve computational efficiency and avoid using loops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Initialize Temperature and Humidity Data\n",
    "We begin by creating two arrays to represent daily climate data for 500 locations across 365 days. The data consists of:\n",
    "- **temperature_data**: Random temperature values (in Celsius) within the range -10°C to 40°C.\n",
    "- **humidity_data**: Random humidity values in percentage, ranging from 0% to 100%.\n",
    "\n",
    "These arrays will allow us to perform analysis and simulations of climate trends across various locations over a year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 365), (500, 365))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate random temperature data in Celsius (-10°C to 40°C)\n",
    "temperature_data = np.random.uniform(-10, 40, (500, 365))\n",
    "\n",
    "# Generate random humidity data in percentage (0% to 100%)\n",
    "humidity_data = np.random.uniform(0, 100, (500, 365))\n",
    "\n",
    "# Display the shapes to verify\n",
    "temperature_data.shape, humidity_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Check for Missing Data\n",
    "To simulate real-world scenarios where data might be incomplete, we randomly set 5% of the entries in both temperature and humidity arrays to `NaN`. We then count the total number of missing entries in each array to confirm the simulation of missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9125, 9125)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate missing data by setting 5% of the values to NaN in both arrays\n",
    "num_missing = int(0.05 * temperature_data.size)\n",
    "temperature_data.flat[np.random.choice(temperature_data.size, num_missing, replace=False)] = np.nan\n",
    "humidity_data.flat[np.random.choice(humidity_data.size, num_missing, replace=False)] = np.nan\n",
    "\n",
    "# Count missing values in each array\n",
    "missing_temperature = np.isnan(temperature_data).sum()\n",
    "missing_humidity = np.isnan(humidity_data).sum()\n",
    "\n",
    "# Output the missing counts\n",
    "missing_temperature, missing_humidity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Convert Temperature and Calculate Discomfort Index\n",
    "1. **Convert Celsius to Fahrenheit**: To facilitate data sharing with international teams, we convert the temperature data from Celsius to Fahrenheit.\n",
    "2. **Calculate \"Feels Like\" Discomfort Index**: We calculate a discomfort index, which combines temperature and humidity to estimate perceived discomfort levels.\n",
    "3. **Cap the Discomfort Index**: Any index value above 80 is capped at 80 to avoid extreme values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[80.        , 80.        , 75.11449784,         nan, 79.96740392],\n",
       "       [80.        , 80.        , 80.        , 80.        ,         nan],\n",
       "       [74.08437348, 80.        , 80.        , 80.        , 80.        ],\n",
       "       [80.        , 18.52749376, 80.        , 80.        , 69.22447565],\n",
       "       [80.        ,         nan, 80.        , 80.        , 80.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert temperature data from Celsius to Fahrenheit\n",
    "temperature_data_fahrenheit = temperature_data * 9/5 + 32\n",
    "\n",
    "# Calculate the discomfort index (basic formula for demonstration purposes)\n",
    "discomfort_index = temperature_data_fahrenheit + 0.5 * humidity_data\n",
    "\n",
    "# Cap discomfort index at 80\n",
    "discomfort_index = np.where(discomfort_index > 80, 80, discomfort_index)\n",
    "\n",
    "# Display the first few values for verification\n",
    "discomfort_index[:5, :5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Analyze January Temperatures\n",
    "We extract the temperature data for January, representing the first 31 days, and calculate the average temperature across all locations. This helps us understand the overall climate pattern in January.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.047620733901034"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract January data (first 31 days)\n",
    "january_temperatures = temperature_data[:, :31]\n",
    "\n",
    "# Calculate the average January temperature across all locations\n",
    "average_january_temperature = np.nanmean(january_temperatures)\n",
    "\n",
    "# Output the average January temperature\n",
    "average_january_temperature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Identify Extreme Temperatures\n",
    "Any temperature values above 35°C are marked as potential errors by setting them to `NaN`. We then count the number of `NaN` values for each location to identify sites with more frequent extreme temperatures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57, 61, 57, 63, 56, 55, 59, 56, 58, 53])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace temperatures above 35°C with NaN to mark as potential error\n",
    "temperature_data[temperature_data > 35] = np.nan\n",
    "\n",
    "# Count the number of NaN values per location (each row)\n",
    "null_counts_per_location = np.isnan(temperature_data).sum(axis=1)\n",
    "\n",
    "# Display the null counts for the first 10 locations for verification\n",
    "null_counts_per_location[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Calculate Quarterly Temperature Averages\n",
    "To examine seasonal trends, we divide the temperature data approximately into four quarters:\n",
    "- Q1: Days 1 to 91\n",
    "- Q2: Days 92 to 182\n",
    "- Q3: Days 183 to 273\n",
    "- Q4: Days 274 to 365\n",
    "\n",
    "We then calculate the average temperature for each location across these quarters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.76313659, 14.09292356,  9.86979986, 10.56241871],\n",
       "       [10.50229151, 12.4561454 , 12.34310543, 11.84997214],\n",
       "       [12.05066887, 12.69032976, 12.82280756, 10.90078526],\n",
       "       [11.8511407 , 14.08559833, 13.43938432, 11.76290684],\n",
       "       [13.66431607, 13.21215313, 12.12261189, 13.8072658 ],\n",
       "       [12.73705857, 11.12872106, 11.78197749, 13.12431132],\n",
       "       [12.04175222, 12.11512203, 11.44652519, 14.77271966],\n",
       "       [14.46117394, 13.0812715 , 10.20498392, 11.66366001],\n",
       "       [12.09085034, 13.45992445, 12.51104235, 11.54377769],\n",
       "       [11.49703219, 13.78533493, 12.1069364 , 11.45966829]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into four approximate quarters\n",
    "Q1 = temperature_data[:, :91]    # First 91 days\n",
    "Q2 = temperature_data[:, 91:182] # Next 91 days\n",
    "Q3 = temperature_data[:, 182:273]# Next 91 days\n",
    "Q4 = temperature_data[:, 273:]   # Remaining days\n",
    "\n",
    "# Calculate the average temperature for each quarter\n",
    "Q1_avg = np.nanmean(Q1, axis=1)\n",
    "Q2_avg = np.nanmean(Q2, axis=1)\n",
    "Q3_avg = np.nanmean(Q3, axis=1)\n",
    "Q4_avg = np.nanmean(Q4, axis=1)\n",
    "\n",
    "# Combine the quarterly averages into a single array for easier analysis\n",
    "quarterly_averages = np.vstack((Q1_avg, Q2_avg, Q3_avg, Q4_avg)).T\n",
    "\n",
    "# Display the first few averages for verification\n",
    "quarterly_averages[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Classify Humidity Levels\n",
    "We categorize each day's humidity level as \"Dry\" if it is below 30% and \"Humid\" if it is above 70%. We then count the total number of \"Dry\" and \"Humid\" days for each location, providing insight into the distribution of humidity levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 96, 112, 110, 108,  95, 121,  98, 101,  96, 104]),\n",
       " array([113, 114, 117, 105, 115,  92, 112, 102, 102, 111]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classify days based on humidity levels\n",
    "dry_days = (humidity_data < 30).sum(axis=1)\n",
    "humid_days = (humidity_data > 70).sum(axis=1)\n",
    "\n",
    "# Display counts for the first 10 locations for verification\n",
    "dry_days[:10], humid_days[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Apply Daily Pressure Trend to Temperature Data\n",
    "In this task, we simulate a daily atmospheric pressure trend (e.g., using a sine wave for simplicity). We apply this trend to adjust daily temperature values at each location, accounting for variations due to atmospheric pressure changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.4406752 , 25.84577152, 20.3107495 ,         nan, 11.52769569],\n",
       "       [12.93019809, 26.29468504, 10.12384678,         nan,         nan],\n",
       "       [15.05315864,  8.90576096,  8.4181725 ,  3.30403173, 15.14347049],\n",
       "       [27.94922774, -9.06067248,         nan, 21.0117078 , 17.96690867],\n",
       "       [        nan,         nan, 13.10872513, 31.13146995, 28.79730544]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a pressure trend across 365 days (sine wave for demonstration)\n",
    "pressure_trend = 5 * np.sin(np.linspace(0, 2 * np.pi, 365))\n",
    "\n",
    "# Adjust temperatures by adding the pressure trend to each day's temperature\n",
    "adjusted_temperature_data = temperature_data + pressure_trend\n",
    "\n",
    "# Display the adjusted temperatures for the first few locations for verification\n",
    "adjusted_temperature_data[:5, :5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a comprehensive approach to analyzing climate data, including temperature and humidity readings across 500 locations for a full year. The tasks covered:\n",
    "- Initializing random climate data,\n",
    "- Simulating missing values,\n",
    "- Converting temperatures and calculating a \"feels like\" index,\n",
    "- Extracting monthly and quarterly trends,\n",
    "- Identifying extreme values,\n",
    "- Classifying humidity levels, and\n",
    "- Applying a daily pressure trend.\n",
    "\n",
    "These analyses showcase basic data manipulation, statistical analysis, and the use of vectorized operations in Python using NumPy. By employing efficient array operations, we were able to perform complex calculations without loops, demonstrating the power of Python for data science tasks.\n",
    "\n",
    "## Repository Information\n",
    "\n",
    "This project is intended as a beginner-friendly example of climate data analysis and will be uploaded to GitHub for further review and sharing. The repository is available at:\n",
    "\n",
    "[GitHub Repository - Climate Data Analysis](https://github.com/shoaib1522/Data-Science-In-Python)\n",
    "\n",
    "For questions or contributions, feel free to contact me via email at sa1670001@gmail.com.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This notebook is designed for Jupyter Notebook use, so all code cells should execute sequentially for accurate results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
