{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📝 **Retail Data Analysis Assignment (Assignment 05)**  \n",
    "**Prepared by: Muhammad Shoaib Ahmad**  \n",
    "**Email: sa1670001@gmail.com**  \n",
    "\n",
    "Welcome to the comprehensive solution to **Assignment 05** for retail data analysis. This document addresses various tasks, including data cleaning, transformations, and generating actionable insights using Python. The tasks involve working with `customers.csv` and `sales.csv` datasets, merging them, and deriving meaningful results to meet business needs.  \n",
    "\n",
    "## Key Features of This Notebook:\n",
    "- 🔍 **Data Inspection:** Analyze and clean real-world retail data.\n",
    "- 🛠️ **Data Transformations:** Perform filtering, grouping, and aggregations for deeper insights.\n",
    "- 📊 **Insight Generation:** Merge datasets and uncover trends, like the city with the highest sales and the product with maximum units sold.\n",
    "- ✅ **Optimized Implementation:** Compare Python dictionaries with DataFrame performance.\n",
    "- 📧 **Contact:** For further queries, feedback, or collaboration, email **sa1670001@gmail.com**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Load and Inspect Data\n",
    "1. Load the `customers.csv` and `sales.csv` files into Pandas DataFrames.\n",
    "2. Display the first few rows of each dataset.\n",
    "3. Show the number of rows and columns in each dataset.\n",
    "4. Check for missing values in both datasets and handle them by:\n",
    "   - Filling missing values in `customers.csv` with \"Unknown.\"\n",
    "   - Filling missing values in `sales.csv` with 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers Dataset Preview:\n",
      "   CustomerID  Age         City\n",
      "0           1   22     New York\n",
      "1           2   23  Los Angeles\n",
      "2           3   24      Chicago\n",
      "3           4   25      Houston\n",
      "4           5   26      Phoenix\n",
      "\n",
      "Sales Dataset Preview:\n",
      "   SaleID  CustomerID     Product  Amount\n",
      "0     101           1      Laptop     200\n",
      "1     102           2  Smartphone     500\n",
      "2     103           3      Tablet     800\n",
      "3     104           4  Headphones    1100\n",
      "4     105           5     Monitor    1400\n",
      "\n",
      "Customers Dataset Shape: (100, 3)\n",
      "Sales Dataset Shape: (400, 4)\n",
      "\n",
      "Checking Missing Values:\n",
      "CustomerID    0\n",
      "Age           0\n",
      "City          0\n",
      "dtype: int64\n",
      "SaleID        0\n",
      "CustomerID    0\n",
      "Product       0\n",
      "Amount        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Loading datasets\n",
    "customers_df = pd.read_csv('customers.csv')\n",
    "sales_df = pd.read_csv('sales.csv')\n",
    "\n",
    "# Displaying initial rows of the datasets\n",
    "print(\"Customers Dataset Preview:\")\n",
    "print(customers_df.head())\n",
    "\n",
    "print(\"\\nSales Dataset Preview:\")\n",
    "print(sales_df.head())\n",
    "\n",
    "# Displaying the shape of the datasets\n",
    "print(\"\\nCustomers Dataset Shape:\", customers_df.shape)\n",
    "print(\"Sales Dataset Shape:\", sales_df.shape)\n",
    "\n",
    "# Checking for missing values\n",
    "print(\"\\nChecking Missing Values:\")\n",
    "print(customers_df.isnull().sum())\n",
    "print(sales_df.isnull().sum())\n",
    "\n",
    "# Handling missing values\n",
    "customers_df.fillna(\"Unknown\", inplace=True)\n",
    "sales_df.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Dictionary Conversion and Filtering\n",
    "1. Convert `customers.csv` into a Python dictionary.\n",
    "2. Filter customers from a specific city (e.g., \"New York\") using the dictionary.\n",
    "3. Repeat the filtering operation using a Pandas DataFrame.\n",
    "4. Compare the efficiency of the dictionary and DataFrame approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of customers in New York using dictionary: 20\n",
      "\n",
      "Number of customers in New York using DataFrame: 20\n"
     ]
    }
   ],
   "source": [
    "# Convert customers data to a dictionary\n",
    "customers_dict = customers_df.to_dict('records')\n",
    "\n",
    "# Filter customers from a specific city (e.g., 'New York')\n",
    "city = \"New York\"\n",
    "filtered_customers_dict = [customer for customer in customers_dict if customer['City'] == city]\n",
    "print(f\"\\nNumber of customers in {city} using dictionary:\", len(filtered_customers_dict))\n",
    "\n",
    "# Filter using DataFrame\n",
    "filtered_customers_df = customers_df[customers_df['City'] == city]\n",
    "print(f\"\\nNumber of customers in {city} using DataFrame:\", len(filtered_customers_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Identifying and Removing Duplicates\n",
    "1. Identify duplicate rows in both datasets.\n",
    "2. Remove these duplicate rows to ensure clean data.\n",
    "3. Verify that no duplicates remain after cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate Rows in Customers Dataset: 0\n",
      "Duplicate Rows in Sales Dataset: 0\n",
      "\n",
      "Duplicates removed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicates\n",
    "print(\"\\nDuplicate Rows in Customers Dataset:\", customers_df.duplicated().sum())\n",
    "print(\"Duplicate Rows in Sales Dataset:\", sales_df.duplicated().sum())\n",
    "\n",
    "# Removing duplicates\n",
    "customers_df.drop_duplicates(inplace=True)\n",
    "sales_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Verifying no duplicates remain\n",
    "assert not customers_df.duplicated().any()\n",
    "assert not sales_df.duplicated().any()\n",
    "print(\"\\nDuplicates removed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Discount Calculation and Grouping\n",
    "1. Add a new column in `sales.csv` to calculate the total amount after applying a 10% discount.\n",
    "2. Group the data by `Product` and calculate the total sales for each product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Sales Per Product (after discount):\n",
      "      Product  DiscountedAmount\n",
      "0  Headphones           79200.0\n",
      "1      Laptop           14400.0\n",
      "2     Monitor          100800.0\n",
      "3  Smartphone           36000.0\n",
      "4      Tablet           57600.0\n"
     ]
    }
   ],
   "source": [
    "# Adding a column for discounted amount\n",
    "sales_df['DiscountedAmount'] = sales_df['Amount'] * 0.9\n",
    "\n",
    "# Grouping by Product and calculating total sales\n",
    "product_sales = sales_df.groupby('Product')['DiscountedAmount'].sum().reset_index()\n",
    "\n",
    "# Displaying the grouped data\n",
    "print(\"\\nTotal Sales Per Product (after discount):\")\n",
    "print(product_sales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Age Filtering and City Analysis\n",
    "1. Filter customers in the age range of 25 to 35 from `customers.csv`.\n",
    "2. Count the number of customers belonging to each city within this age range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Customer Counts by City (Age 25-35):\n",
      "City\n",
      "Houston        11\n",
      "Phoenix         8\n",
      "New York        7\n",
      "Los Angeles     7\n",
      "Chicago         7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filtering customers by age\n",
    "filtered_customers_age = customers_df[(customers_df['Age'] >= 25) & (customers_df['Age'] <= 35)]\n",
    "\n",
    "# Counting customers by city\n",
    "city_customer_counts = filtered_customers_age['City'].value_counts()\n",
    "\n",
    "print(\"\\nCustomer Counts by City (Age 25-35):\")\n",
    "print(city_customer_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Merging and Insights\n",
    "1. Merge `customers.csv` and `sales.csv` datasets on `CustomerID`.\n",
    "2. Identify:\n",
    "   - The city generating the highest total sales.\n",
    "   - The product with the most units sold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City with Highest Total Sales: Phoenix\n",
      "Product with Most Units Sold: Laptop\n"
     ]
    }
   ],
   "source": [
    "# Merging datasets\n",
    "merged_df = pd.merge(customers_df, sales_df, on='CustomerID')\n",
    "\n",
    "# Insights about the city with the highest total sales\n",
    "highest_sales_city = merged_df.groupby('City')['DiscountedAmount'].sum().idxmax()\n",
    "print(\"City with Highest Total Sales:\", highest_sales_city)\n",
    "\n",
    "# Most sold product based on occurrence in the Product column\n",
    "most_sold_product = merged_df['Product'].value_counts().idxmax()\n",
    "print(\"Product with Most Units Sold:\", most_sold_product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Unique Values and Statistical Analysis\n",
    "1. Display the unique values in the `City` and `Product` columns.\n",
    "2. Calculate the mean and median of the `Amount` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique Cities: ['New York' 'Los Angeles' 'Chicago' 'Houston' 'Phoenix']\n",
      "Unique Products: ['Laptop' 'Smartphone' 'Tablet' 'Headphones' 'Monitor']\n",
      "\n",
      "Mean of Amount: 800.0\n",
      "Median of Amount: 800.0\n"
     ]
    }
   ],
   "source": [
    "# Exploring unique values\n",
    "unique_cities = merged_df['City'].unique()\n",
    "unique_products = merged_df['Product'].unique()\n",
    "\n",
    "print(\"\\nUnique Cities:\", unique_cities)\n",
    "print(\"Unique Products:\", unique_products)\n",
    "\n",
    "# Calculating statistics\n",
    "mean_amount = merged_df['Amount'].mean()\n",
    "median_amount = merged_df['Amount'].median()\n",
    "\n",
    "print(\"\\nMean of Amount:\", mean_amount)\n",
    "print(\"Median of Amount:\", median_amount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ✨ **Conclusion**\n",
    "\n",
    "This notebook successfully completes all tasks outlined in Assignment 05. Through a structured approach to data analysis, we achieved the following key outcomes:\n",
    "\n",
    "1. **Data Cleaning:** Missing values and duplicates were identified and handled effectively, ensuring data integrity.\n",
    "2. **Performance Comparison:** Filtering operations were compared using Python dictionaries and Pandas DataFrames, highlighting the efficiency of modern libraries for large-scale datasets.\n",
    "3. **Actionable Insights:** By applying business logic, such as calculating discounts and analyzing city and product trends, we derived insights valuable for decision-making.\n",
    "4. **Statistical Analysis:** Unique values, means, and medians of critical columns were calculated for a comprehensive overview.\n",
    "\n",
    "---\n",
    "\n",
    "### **Contact for Queries**\n",
    "If you have any questions, feedback, or suggestions, please feel free to contact me at:\n",
    "\n",
    "📧 **Email:** [sa1670001@gmail.com](mailto:sa1670001@gmail.com)  \n",
    "\n",
    "I look forward to connecting and discussing any aspects of this project further!\n",
    "\n",
    "--- \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
